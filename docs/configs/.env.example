# ===============================
# AI Review .env example
# ===============================

# --- LLM ---
LLM__PROVIDER=OPENAI
LLM__PRICING_FILE=./pricing.yaml

# --- OpenAI specific ---
LLM__META__MODEL=gpt-4o-mini
LLM__META__MAX_TOKENS=1200
LLM__META__TEMPERATURE=0.3

# --- Gemini alternative ---
# LLM__META__MODEL=gemini-2.0-pro
# LLM__META__MAX_TOKENS=1200
# LLM__META__TEMPERATURE=0.3

# --- Claude alternative ---
# LLM__META__MODEL=claude-3-sonnet
# LLM__META__MAX_TOKENS=1200
# LLM__META__TEMPERATURE=0.3

# --- Ollama alternative ---
# LLM__PROVIDER=OLLAMA
# LLM__META__MODEL=llama2
# LLM__META__MAX_TOKENS=512
# LLM__META__TEMPERATURE=0.3
# LLM__META__TOP_P=0.9
# LLM__META__REPEAT_PENALTY=1.1
# LLM__META__STOP="USER:,SYSTEM:"
# LLM__META__SEED=42

# --- HTTP client for LLM ---
LLM__HTTP_CLIENT__TIMEOUT=120
LLM__HTTP_CLIENT__API_URL=https://api.openai.com/v1
LLM__HTTP_CLIENT__API_TOKEN=${OPENAI_API_KEY}

# For Claude (only if provider=CLAUDE)
# LLM__HTTP_CLIENT__API_URL=https://api.anthropic.com
# LLM__HTTP_CLIENT__API_TOKEN=${CLAUDE_API_KEY}
# LLM__HTTP_CLIENT__API_VERSION=2023-06-01

# For Gemini (only if provider=GEMINI)
# LLM__HTTP_CLIENT__API_URL=https://generativelanguage.googleapis.com
# LLM__HTTP_CLIENT__API_TOKEN=${GEMINI_API_KEY}

# For Ollama (only if provider=OLLAMA, no API token required)
# LLM__HTTP_CLIENT__API_URL=http://localhost:11434

# --- VCS ---
VCS__PROVIDER=GITLAB

# --- GitLab ---
VCS__PIPELINE__PROJECT_ID=${CI_PROJECT_ID}
VCS__PIPELINE__MERGE_REQUEST_ID=${CI_MERGE_REQUEST_IID}

VCS__HTTP_CLIENT__TIMEOUT=120
VCS__HTTP_CLIENT__API_URL=${CI_SERVER_URL}
VCS__HTTP_CLIENT__API_TOKEN=${CI_JOB_TOKEN}

# --- GitHub ---
# VCS__PROVIDER=GITHUB
#
# VCS__PIPELINE__OWNER=${GITHUB_REPOSITORY_OWNER}
# VCS__PIPELINE__REPO=${GITHUB_REPOSITORY_NAME}
# VCS__PIPELINE__PULL_NUMBER=${PR_NUMBER}
#
# VCS__HTTP_CLIENT__TIMEOUT=120
# VCS__HTTP_CLIENT__API_URL=https://api.github.com
# VCS__HTTP_CLIENT__API_TOKEN=${GITHUB_TOKEN}


# --- Bitbucket ---
# VCS__PROVIDER=BITBUCKET
#
# VCS__PIPELINE__WORKSPACE=${BITBUCKET_WORKSPACE}
# VCS__PIPELINE__REPO_SLUG=${BITBUCKET_REPO_SLUG}
# VCS__PIPELINE__PULL_REQUEST_ID=${BITBUCKET_PR_ID}
#
# VCS__HTTP_CLIENT__TIMEOUT=120
# VCS__HTTP_CLIENT__API_URL=https://api.bitbucket.org/2.0
# VCS__HTTP_CLIENT__API_TOKEN=${BITBUCKET_TOKEN}

# --- Core ---
CORE__CONCURRENCY=7

# --- Prompts ---
PROMPT__CONTEXT__ENVIRONMENT=staging
PROMPT__CONTEXT__COMPANY_NAME="ACME Corp"
PROMPT__CONTEXT__CI_PIPELINE_URL=https://gitlab.com/pipelines/123

PROMPT__CONTEXT_PLACEHOLDER="<<{value}>>"

PROMPT__NORMALIZE_PROMPTS=true

PROMPT__INLINE_PROMPT_FILES='["./prompts/default_inline.md"]'
PROMPT__CONTEXT_PROMPT_FILES='["./prompts/default_context.md"]'
PROMPT__SUMMARY_PROMPT_FILES='["./prompts/default_summary.md"]'

PROMPT__SYSTEM_INLINE_PROMPT_FILES='["./prompts/default_system_inline.md"]'
PROMPT__SYSTEM_CONTEXT_PROMPT_FILES='["./prompts/default_system_context.md"]'
PROMPT__SYSTEM_SUMMARY_PROMPT_FILES='["./prompts/default_system_summary.md"]'

PROMPT__INCLUDE_INLINE_SYSTEM_PROMPTS=true
PROMPT__INCLUDE_CONTEXT_SYSTEM_PROMPTS=true
PROMPT__INCLUDE_SUMMARY_SYSTEM_PROMPTS=true

# --- Review ---
# Available modes:
#   FULL_FILE_DIFF
#   FULL_FILE_CURRENT
#   FULL_FILE_PREVIOUS
#   ONLY_ADDED
#   ONLY_REMOVED
#   ADDED_AND_REMOVED
#   ONLY_ADDED_WITH_CONTEXT
#   ONLY_REMOVED_WITH_CONTEXT
#   ADDED_AND_REMOVED_WITH_CONTEXT
REVIEW__MODE=FULL_FILE_DIFF

REVIEW__INLINE_TAG="#ai-review-inline"
REVIEW__SUMMARY_TAG="#ai-review-summary"
REVIEW__CONTEXT_LINES=10
REVIEW__ALLOW_CHANGES=
REVIEW__IGNORE_CHANGES=
REVIEW__MAX_INLINE_COMMENTS=
REVIEW__MAX_CONTEXT_COMMENTS=
REVIEW__REVIEW_ADDED_MARKER=" # added"
REVIEW__REVIEW_REMOVED_MARKER=" # removed"

# --- Logger ---
# Options: NOTSET | DEBUG | INFO | WARNING | ERROR | CRITICAL
LOGGER__LEVEL=INFO
LOGGER__FORMAT="{time:YYYY-MM-DD HH:mm:ss} | {level} | {extra[logger_name]} | {message}"

# --- Artifacts ---
ARTIFACTS__LLM_DIR=./artifacts/llm
ARTIFACTS__LLM_ENABLED=false
