# ===============================
# AI Review configuration file
# ===============================

llm:
  # Provider for the Large Language Model.
  # Options: OPENAI | GEMINI | CLAUDE | OLLAMA
  provider: OPENAI

  # Path to the pricing file used for cost estimation.
  # Default: ./pricing.yaml
  pricing_file: ./pricing.yaml

  # LLM metadata configuration (depends on provider).
  meta:
    # --- OpenAI ---
    model: gpt-4o-mini  # e.g., gpt-4o, gpt-4o-mini, gpt-3.5-turbo
    max_tokens: 1200    # Maximum tokens for completion
    temperature: 0.3    # Creativity of responses (0 = deterministic)

    # --- Gemini ---
    # model: gemini-2.0-pro
    # max_tokens: 1200
    # temperature: 0.3

    # --- Claude ---
    # model: claude-3-sonnet   # e.g., claude-3-opus, claude-3-haiku
    # max_tokens: 1200
    # temperature: 0.3

    # --- Ollama ---
    # model: llama2               # e.g., llama2, mistral, codellama
    # max_tokens: 512             # Maps to `num_predict`
    # temperature: 0.3
    # top_p: 0.9                  # (optional) nucleus sampling
    # repeat_penalty: 1.1         # (optional) repetition penalty
    # stop: ["USER:", "SYSTEM:"]  # (optional) stop sequences
    # seed: 42                    # (optional) random seed for reproducibility

  # HTTP client configuration for LLM.
  http_client:
    timeout: 120                        # Request timeout in seconds
    api_url: https://api.openai.com/v1  # Base URL of the provider API
    api_token: ${OPENAI_API_KEY}        # API token (set in env/CI variables)

    # --- Gemini ---
    # api_url: https://generativelanguage.googleapis.com
    # api_token: ${GEMINI_API_KEY}

    # --- Claude ---
    # api_url: https://api.anthropic.com
    # api_token: ${CLAUDE_API_KEY}
    # api_version: 2023-06-01

    # --- Ollama ---
    # api_url: http://localhost:11434   # Local Ollama runtime
    # (no api_token required)

vcs:
  # Version control system provider.
  # Options: GITLAB | GITHUB | BITBUCKET
  provider: GITLAB

  # Pipeline/MR/PR context.
  pipeline:
    # --- GitLab ---
    project_id: ${CI_PROJECT_ID}               # GitLab project ID (auto-populated in CI)
    merge_request_id: ${CI_MERGE_REQUEST_IID}  # Merge Request IID (auto-populated in CI)

    # --- GitHub ---
    # owner: ${GITHUB_REPOSITORY_OWNER}  # Repo owner (e.g. "my-org")
    # repo: ${GITHUB_REPOSITORY_NAME}    # Repo name  (e.g. "my-service")
    # pull_number: ${PR_NUMBER}          # Pull Request number

    # --- Bitbucket ---
    # workspace: ${BITBUCKET_WORKSPACE}         # Workspace ID (e.g. "myteam")
    # repo_slug: ${BITBUCKET_REPO_SLUG}         # Repository slug (short name, e.g. "my-service")
    # pull_request_id: ${BITBUCKET_PR_ID}       # Pull Request ID (numeric, e.g. "42")

  # HTTP client configuration for VCS.
  http_client:
    timeout: 120

    # --- GitLab ---
    api_url: ${CI_SERVER_URL}       # Base GitLab server URL
    api_token: ${CI_JOB_TOKEN}      # Job token or personal access token

    # --- GitHub ---
    # api_url: https://api.github.com
    # api_token: ${GITHUB_TOKEN}      # GitHub Actions token or PAT

    # --- Bitbucket ---
    # api_url: https://api.bitbucket.org/2.0
    # api_token: ${BITBUCKET_TOKEN}   # App password or OAuth token

  # Pagination settings for API calls.
  pagination:
    per_page: 100  # Number of items per API page (default: 100)
    max_pages: 5   # Maximum number of pages to fetch before stopping (default: 5)

core:
  # ==============================
  # Core engine configuration
  # ==============================
  # Controls the maximum number of concurrently running async tasks.
  # Used in bounded_gather() to limit concurrency when sending requests
  # (e.g., posting comments or fetching data from LLM/VCS APIs).
  #
  # Increasing this value allows more operations in parallel,
  # but may hit API rate limits or increase memory consumption.
  #
  # Recommended: 5–10 for LLM requests, depending on model rate limits.
  concurrency: 7

prompt:
  # ==============================
  # Custom context variables
  # ==============================
  # context (key-value map).
  # These variables are injected into all prompts using placeholders.
  # Useful for adding project-specific metadata (company, team handles, CI/CD links, etc.).
  #
  # Referenced in prompt templates:
  #   Project: <<company_name>>
  #   Env: <<environment>>
  #   Pipeline: <<ci_pipeline_url>>
  #
  # These values override built-in variables if names collide.
  # To avoid clashes, prefer namespaced keys (ci_pipeline_url, org_notify_handle, env_name).
  context:
    environment: "staging"
    company_name: "ACME Corp"
    ci_pipeline_url: "https://gitlab.com/pipelines/123"

  # ==============================
  # Context placeholder
  # ==============================
  # Defines how placeholders are written in prompt templates.
  # Must contain "{value}" which will be replaced by the variable name.
  #
  # Default: <<{value}>>
  #
  # Example:
  #   context_placeholder: "<<{value}>>"
  #   Template: "Env: <<environment>>"
  #   Result:   "Env: staging"
  context_placeholder: "<<{value}>>"

  # ==============================
  # Prompt normalization
  # ==============================
  # Whether to normalize prompts before sending to the LLM.
  #
  # true  (default) →
  #   - strips trailing spaces/tabs at the end of each line,
  #   - collapses runs of 3+ empty lines into a single blank line,
  #   - trims leading/trailing whitespace of the entire prompt.
  #
  # false →
  #   - prompts are passed "as is",
  #   - useful if whitespace and formatting are significant
  #     (e.g. testing model sensitivity, preserving custom layout).
  normalize_prompts: true

  # ==============================
  # Inline prompts
  # ==============================
  # inline_prompt_files (joined in order).
  # These provide *local instructions* for inline review,
  # e.g. what to check in code (readability, bugs, style, etc.).
  #
  # Defaults (if not defined):
  #   - default_inline.md
  #
  inline_prompt_files:
    - ./prompts/default_inline.md  # MR-specific inline prompt

  # system_inline_prompt_files (joined in order).
  # These define the *contract* with the model for inline review:
  # - enforce JSON output (file, line, message, suggestion),
  # - restrict scope (only changed lines, no extras),
  # - define comment structure rules.
  #
  # Defaults (if not defined):
  #   - default_system_inline.md
  #
  system_inline_prompt_files:
    - ./prompts/default_system_inline.md  # System inline prompt

  # Whether to include built-in default system inline prompts.
  # true  (default) → always prepend default_system_inline.md
  # false → only user-provided system_inline_prompt_files are used
  include_inline_system_prompts: true

  # ==============================
  # Context prompts
  # ==============================
  # context_prompt_files (joined in order).
  # These provide *local instructions* for context review,
  # e.g. broader analysis across multiple files, naming consistency,
  # architectural issues, or duplicated code.
  #
  # Defaults (if not defined):
  #   - default_context.md
  #
  context_prompt_files:
    - ./prompts/default_context.md  # MR-specific context prompt

  # system_context_prompt_files (joined in order).
  # These define the *contract* with the model for context review:
  # - enforce JSON output (same format as inline),
  # - restrict scope (only changed files, no extras),
  # - define comment structure rules.
  #
  # Defaults (if not defined):
  #   - default_system_context.md
  #
  system_context_prompt_files:
    - ./prompts/default_system_context.md  # System context prompt

  # Whether to include built-in default system context prompts.
  # true  (default) → always prepend default_system_context.md
  # false → only user-provided system_context_prompt_files are used
  include_context_system_prompts: true

  # ==============================
  # Summary prompts
  # ==============================
  # summary_prompt_files (joined in order).
  # These provide *local instructions* for summary review,
  # e.g. how to write the MR overview, highlight issues and strengths.
  #
  # Defaults (if not defined):
  #   - default_summary.md
  #
  summary_prompt_files:
    - ./prompts/default_summary.md  # MR-specific summary prompt

  # system_summary_prompt_files (joined in order).
  # These define the *contract* with the model for summary review:
  # - enforce plain-text output,
  # - restrict to 1–4 sentences,
  # - return "No issues found." if nothing to report.
  #
  # Defaults (if not defined):
  #   - default_system_summary.md
  #
  system_summary_prompt_files:
    - ./prompts/default_system_summary.md  # System summary prompt

  # Whether to include built-in default system summary prompts.
  # true  (default) → always prepend default_system_summary.md
  # false → only user-provided system_summary_prompt_files are used
  include_summary_system_prompts: true

  # ==============================
  # Inline reply prompts
  # ==============================
  # inline_reply_prompt_files (joined in order).
  # These provide *local instructions* for AI reply generation
  # inside existing inline comment threads.
  #
  # Useful when the AI should continue discussions or explain prior feedback.
  #
  # Defaults (if not defined):
  #   - default_inline_reply.md
  #
  inline_reply_prompt_files:
    - ./prompts/default_inline_reply.md  # MR-specific inline reply prompt

  # system_inline_reply_prompt_files (joined in order).
  # These define the *contract* with the model for inline reply generation:
  # - enforce plain-text output (single reply per thread),
  # - restrict replies only to comments authored by AI Review,
  # - define response tone and content structure.
  #
  # Defaults (if not defined):
  #   - default_system_inline_reply.md
  #
  system_inline_reply_prompt_files:
    - ./prompts/default_system_inline_reply.md  # System inline reply prompt

  # Whether to include built-in default system inline reply prompts.
  # true  (default) → always prepend default_system_inline_reply.md
  # false → only user-provided system_inline_reply_prompt_files are used
  include_inline_reply_system_prompts: true

  # ==============================
  # Summary reply prompts
  # ==============================
  # summary_reply_prompt_files (joined in order).
  # These provide *local instructions* for generating AI replies
  # to summary review comments (e.g. reviewer discussion threads).
  #
  # Useful for collaborative or follow-up review workflows.
  #
  # Defaults (if not defined):
  #   - default_summary_reply.md
  #
  summary_reply_prompt_files:
    - ./prompts/default_summary_reply.md  # MR-specific summary reply prompt

  # system_summary_reply_prompt_files (joined in order).
  # These define the *contract* with the model for summary reply generation:
  # - enforce plain-text output,
  # - restrict replies to summary threads started by AI Review,
  # - define reply tone (short, factual, concise).
  #
  # Defaults (if not defined):
  #   - default_system_summary_reply.md
  #
  system_summary_reply_prompt_files:
    - ./prompts/default_system_summary_reply.md  # System summary reply prompt

  # Whether to include built-in default system summary reply prompts.
  # true  (default) → always prepend default_system_summary_reply.md
  # false → only user-provided system_summary_reply_prompt_files are used
  include_summary_reply_system_prompts: true

review:
  # Review mode determines which parts of files are reviewed.
  # Options:
  #   FULL_FILE_DIFF              - Compare added & removed lines (default)
  #   FULL_FILE_CURRENT           - Review current file version
  #   FULL_FILE_PREVIOUS          - Review previous file version
  #
  #   ONLY_ADDED                  - Only added lines
  #   ONLY_REMOVED                - Only removed lines
  #   ADDED_AND_REMOVED           - Both added and removed lines
  #
  #   ONLY_ADDED_WITH_CONTEXT     - Added lines + N lines of context
  #   ONLY_REMOVED_WITH_CONTEXT   - Removed lines + N lines of context
  #   ADDED_AND_REMOVED_WITH_CONTEXT - Both added/removed lines + N lines of context
  mode: FULL_FILE_DIFF

  # Tags used to mark AI-generated comments in MR.
  inline_tag: "#ai-review-inline"
  inline_reply_tag: "#ai-review-inline-reply"
  summary_tag: "#ai-review-summary"
  summary_reply_tag: "#ai-review-summary-reply"

  # Number of context lines around changes (only used in *_WITH_CONTEXT modes).
  context_lines: 10

  # File filters.
  allow_changes: [ ]   # List of glob patterns to explicitly allow review (empty = all allowed)
  ignore_changes: [ ]  # List of glob patterns to ignore, e.g. ["docs/*", "README.md"]

  # Marker used to annotate changed lines in output.
  review_added_marker: " # added"
  review_removed_marker: " # removed"

  # Limit number of AI-generated comments to avoid noise.
  # If None → no limit is applied.
  max_inline_comments: null   # Maximum comments for inline review (per file)
  max_context_comments: null  # Maximum comments for context review (per MR)

logger:
  # Logging level for the application.
  # Options: NOTSET | DEBUG | INFO | WARNING | ERROR | CRITICAL
  level: INFO

  # Format string for log messages.
  format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {extra[logger_name]} | {message}"

artifacts:
  # Directory for storing LLM artifacts (created automatically if missing).
  llm_dir: ./artifacts/llm

  # Whether to enable saving/loading artifacts from this directory.
  llm_enabled: false