.ai-review-base:
  tags: [ build ]
  when: manual
  stage: review
  image: nikitafilonov/ai-review:latest
  rules:
    - if: '$CI_MERGE_REQUEST_IID'
  script:
    - ai-review run
  variables:
    # Required: fetch full history so that git diff between base and head SHAs works correctly.
    GIT_DEPTH: 0

    # ===============================
    # LLM provider & model
    # ===============================
    # Which LLM to use.
    # Options: OPENAI | GEMINI | CLAUDE | OLLAMA | OPENROUTER
    LLM__PROVIDER: "OPENAI"

    # --- Model metadata ---
    # For OpenAI: gpt-4o, gpt-4o-mini, gpt-3.5-turbo, gpt-4.1, gpt-4.1-mini, gpt-5
    # For Gemini: gemini-2.0-pro, gemini-2.0-flash
    # For Claude: claude-3-opus, claude-3-sonnet, claude-3-haiku
    # For Ollama: llama2, codellama, mistral, etc.
    # For OpenRouter: anthropic/claude-3.5-sonnet, meta-llama/llama-3-70b-instruct, mistralai/mixtral-8x7b, etc.
    LLM__META__MODEL: "gpt-4o-mini"

    # (optional) Max tokens for completion.
    LLM__META__MAX_TOKENS: "15000"

    # (optional) Creativity of responses (0 = deterministic, >0 = more creative).
    LLM__META__TEMPERATURE: "0.3"

    # --- HTTP client configuration ---
    # API endpoint + token (must be set as CI/CD variables).
    LLM__HTTP_CLIENT__API_URL: "https://api.openai.com/v1"
    LLM__HTTP_CLIENT__API_TOKEN: "$OPENAI_API_KEY"

    # Example for Gemini:
    # LLM__HTTP_CLIENT__API_URL: "https://generativelanguage.googleapis.com"
    # LLM__HTTP_CLIENT__API_TOKEN: "$GEMINI_API_KEY"

    # Example for Claude:
    # LLM__HTTP_CLIENT__API_URL: "https://api.anthropic.com"
    # LLM__HTTP_CLIENT__API_TOKEN: "$CLAUDE_API_KEY"
    # LLM__HTTP_CLIENT__API_VERSION: "2023-06-01"

    # Example for Ollama (no token required):
    # LLM__HTTP_CLIENT__API_URL: "http://localhost:11434"

    # Example for OpenRouter:
    # LLM__HTTP_CLIENT__API_URL: "https://openrouter.ai/api/v1"
    # LLM__HTTP_CLIENT__API_TOKEN: "$OPENROUTER_API_KEY"

    # ===============================
    # VCS (GitLab integration)
    # ===============================
    VCS__PROVIDER: "GITLAB"

    # Context of the current pipeline (auto-populated by GitLab).
    VCS__PIPELINE__PROJECT_ID: "$CI_PROJECT_ID"
    VCS__PIPELINE__MERGE_REQUEST_ID: "$CI_MERGE_REQUEST_IID"

    # GitLab API access.

    # Note:
    # CI_JOB_TOKEN is the default and safest choice for most GitLab pipelines.
    # However, on some self-hosted instances or with limited API permissions,
    # it may not allow reading/writing merge request discussions or notes.
    #
    # If you see "401 Unauthorized" errors in logs, replace it with
    # a Personal Access Token or Bot Token that has the "api" scope:
    #
    #   VCS__HTTP_CLIENT__API_TOKEN: "$GITLAB_API_TOKEN"
    #
    # Reference:
    #   https://docs.gitlab.com/ci/jobs/ci_job_token/#job-token-access
    VCS__HTTP_CLIENT__API_URL: "$CI_SERVER_URL"
    VCS__HTTP_CLIENT__API_TOKEN: "$CI_JOB_TOKEN"

    # ===============================
    # Core (execution settings)
    # ===============================
    # Limits the maximum number of concurrent async tasks
    # (e.g., posting comments or calling LLMs).
    # Helps avoid hitting API rate limits.
    #
    # Default: 7
    #
    # Example:
    #   CORE__CONCURRENCY: 5
    #
    # Recommended range: 5â€“10 depending on model/API limits.
    CORE__CONCURRENCY: 7

    # ===============================
    # Prompts (optional overrides)
    # ===============================
    # Whether to normalize prompts before sending to the LLM.
    # Controls stripping trailing spaces, collapsing multiple blank lines,
    # and trimming leading/trailing whitespace.
    #
    # Default: "true"
    #
    # Example:
    #   PROMPT__NORMALIZE_PROMPTS: "false"
    #
    # Set to "false" if you want prompts to be passed to the model "as is",
    # without any cleanup or formatting changes.
    #
    # PROMPT__NORMALIZE_PROMPTS: "true"

    # Inline prompts (joined in order, local review instructions).
    # PROMPT__INLINE_PROMPT_FILES: '["./prompts/inline.md"]'

    # Inline system prompts (format/contract rules).
    # PROMPT__SYSTEM_INLINE_PROMPT_FILES: '["./prompts/system_inline.md"]'
    # PROMPT__INCLUDE_INLINE_SYSTEM_PROMPTS: "true"

    # Context prompts (joined in order, broader analysis instructions).
    # PROMPT__CONTEXT_PROMPT_FILES: '["./prompts/context.md"]'

    # Context system prompts (format/contract rules).
    # PROMPT__SYSTEM_CONTEXT_PROMPT_FILES: '["./prompts/system_context.md"]'
    # PROMPT__INCLUDE_CONTEXT_SYSTEM_PROMPTS: "true"

    # Summary prompts (joined in order, local review instructions).
    # PROMPT__SUMMARY_PROMPT_FILES: '["./prompts/summary.md"]'

    # Summary system prompts (format/contract rules).
    # PROMPT__SYSTEM_SUMMARY_PROMPT_FILES: '["./prompts/system_summary.md"]'
    # PROMPT__INCLUDE_SUMMARY_SYSTEM_PROMPTS: "true"

    # Inline reply prompts (used when replying to inline code review comments).
    # PROMPT__INLINE_REPLY_PROMPT_FILES: '["./prompts/inline_reply.md"]'

    # Inline reply system prompts (format/contract rules).
    # PROMPT__SYSTEM_INLINE_REPLY_PROMPT_FILES: '["./prompts/system_inline_reply.md"]'
    # PROMPT__INCLUDE_INLINE_REPLY_SYSTEM_PROMPTS: "true"

    # Summary reply prompts (used when replying in summary review threads).
    # PROMPT__SUMMARY_REPLY_PROMPT_FILES: '["./prompts/summary_reply.md"]'

    # Summary reply system prompts (format/contract rules).
    # PROMPT__SYSTEM_SUMMARY_REPLY_PROMPT_FILES: '["./prompts/system_summary_reply.md"]'
    # PROMPT__INCLUDE_SUMMARY_REPLY_SYSTEM_PROMPTS: "true"

    # ===============================
    # Custom context variables
    # ===============================
    # You can inject custom variables into prompts via PROMPT__CONTEXT__*.
    # These will be available in all templates through placeholders.
    #
    # Placeholder syntax is defined separately in PROMPT__CONTEXT_PLACEHOLDER.
    # Default: <<{value}>>
    #
    # Example usage in prompt templates:
    #   Project: <<company_name>>
    #   Env: <<environment>>
    #   Pipeline: <<ci_pipeline_url>>
    #
    # Values override built-in variables if names collide.
    # To avoid clashes, prefer namespaced keys
    # (ci_pipeline_url, org_notify_handle, env_name).
    #
    # PROMPT__CONTEXT__ENVIRONMENT: "staging"
    # PROMPT__CONTEXT__COMPANY_NAME: "ACME Corp"
    # PROMPT__CONTEXT__CI_PIPELINE_URL: "https://gitlab.com/pipelines/123"
    #
    # ===============================
    # Context placeholder
    # ===============================
    # Defines how placeholders are written in prompt templates.
    # Must contain "{value}" which will be replaced by the variable name.
    #
    # Default: <<{value}>>
    #
    # Example:
    #   PROMPT__CONTEXT_PLACEHOLDER: "<<{value}>>"
    #   Template: "Env: <<environment>>"
    #   Result:   "Env: staging"
    #
    # PROMPT__CONTEXT_PLACEHOLDER: "<<{value}>>"

    # ===============================
    # Review options
    # ===============================
    # Available modes:
    #   FULL_FILE_DIFF
    #   FULL_FILE_CURRENT
    #   FULL_FILE_PREVIOUS
    #   ONLY_ADDED
    #   ONLY_REMOVED
    #   ADDED_AND_REMOVED
    #   ONLY_ADDED_WITH_CONTEXT
    #   ONLY_REMOVED_WITH_CONTEXT
    #   ADDED_AND_REMOVED_WITH_CONTEXT
    REVIEW__MODE: "FULL_FILE_DIFF"

    # Dry-run mode:
    # When enabled, the review runs fully but does NOT create
    # any comments or replies in the VCS (GitHub / GitLab / Bitbucket / Gitea).
    # Useful for local testing, CI preview, or validation runs.
    # Example: REVIEW__DRY_RUN: "true"
    # Default: "false"
    REVIEW__DRY_RUN: "false"

    # Tags used to mark AI-generated comments in MR.
    REVIEW__INLINE_TAG: "#ai-review-inline"
    REVIEW__INLINE_REPLY_TAG: "#ai-review-inline-reply"
    REVIEW__SUMMARY_TAG: "#ai-review-summary"
    REVIEW__SUMMARY_REPLY_TAG: "#ai-review-summary-reply"

    # Context lines (only for *_WITH_CONTEXT modes).
    REVIEW__CONTEXT_LINES: "10"

    # Markers for changes in output.
    REVIEW__REVIEW_ADDED_MARKER: " # added"
    REVIEW__REVIEW_REMOVED_MARKER: " # removed"

    # Optional filters:
    # REVIEW__ALLOW_CHANGES: "src/*,lib/*"
    # REVIEW__IGNORE_CHANGES: "docs/*,README.md"

    # Optional limits for number of AI comments:
    # REVIEW__MAX_INLINE_COMMENTS: "20"   # Max inline comments per file (default: unlimited)
    # REVIEW__MAX_CONTEXT_COMMENTS: "50"  # Max context comments per MR (default: unlimited)

    # ===============================
    # Logger (optional)
    # ===============================
    LOGGER__LEVEL: "INFO"
    LOGGER__FORMAT: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {extra[logger_name]} | {message}"

    # ===============================
    # Artifacts (optional)
    # ===============================
    ARTIFACTS__LLM_DIR: "./artifacts/llm"
    ARTIFACTS__LLM_ENABLED: "false"

  allow_failure: true  # Optional: don't block pipeline if AI review fails


.ai-review-inline:
  script:
    - ai-review run-inline
  extends: .ai-review-base

.ai-review-context:
  script:
    - ai-review run-context
  extends: .ai-review-base

.ai-review-summary:
  script:
    - ai-review run-summary
  extends: .ai-review-base

.ai-review-inline-reply:
  script:
    - ai-review run-inline-reply
  extends: .ai-review-base

.ai-review-summary-reply:
  script:
    - ai-review run-summary-reply
  extends: .ai-review-base