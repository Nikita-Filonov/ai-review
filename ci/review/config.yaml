llm:
  provider: OPENAI

  meta:
    model: gpt-4o-mini
    max_tokens: 15000
    temperature: 0.3

  http_client:
    timeout: 120
    api_url: https://api.openai.com/v1

vcs:
  provider: GITHUB

  http_client:
    timeout: 120
    api_url: https://api.github.com

prompt:
  inline_prompt_files: [ ./docs/prompts/python/inline/strict.md ]
  context_prompt_files: [ ./docs/prompts/python/inline/strict.md ]
  summary_prompt_files: [ ./docs/prompts/python/summary/strict.md ]
  inline_reply_prompt_files: [ ./docs/prompts/python/inline_reply/strict.md ]
  summary_reply_prompt_files: [ ./docs/prompts/python/summary_reply/strict.md ]

artifacts:
  llm_dir: ./artifacts/llm
  llm_enabled: true